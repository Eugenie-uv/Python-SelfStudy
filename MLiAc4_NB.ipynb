{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1：文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T08:20:14.465382Z",
     "start_time": "2019-11-07T08:20:14.449782Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:41:38.428361Z",
     "start_time": "2019-11-07T09:41:38.397161Z"
    }
   },
   "outputs": [],
   "source": [
    "# 实验样本：词条切割后的集合\n",
    "def loadDataSet():\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problem', 'help', 'please'],\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worhless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0, 1, 0, 1 , 0, 1] #1有侮辱性文字 0正常言论\n",
    "    return postingList, classVec\n",
    "\n",
    "# 创建词汇集合列表\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([]) #集合\n",
    "    for doc in dataSet:\n",
    "        vocabSet = vocabSet | set(doc) #并集\n",
    "    return list(vocabSet)\n",
    "\n",
    "# 词集模型set()：将给定词组转换成 在词汇表出现的标记1或0\n",
    "# 只在乎出现/不出现\n",
    "def Words2Vec(vocabList, inputSet):\n",
    "    res = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            res[vocabList.index(word)] = 1 #出现该文字的位置标记为1\n",
    "        else: print('The word \"%s\" is not in my Vocabulary.'%word) #没有出现的单词打印出来\n",
    "    return res\n",
    "\n",
    "# 词袋模型\n",
    "# care词频\n",
    "def bagWords2Vec(vocabList, inputSet):\n",
    "    res = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        res[vocabList.index(word)] += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T07:41:57.096147Z",
     "start_time": "2019-11-07T07:41:57.064947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'dog', 'has', 'flea', 'problem', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worhless', 'dog', 'food', 'stupid']]\n",
      "[0, 1, 0, 1, 0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listPost, listClass = loadDataSet()\n",
    "print(listPost), print(listClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T07:44:47.947647Z",
     "start_time": "2019-11-07T07:44:47.916447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['please', 'to', 'ate', 'not', 'how', 'him', 'flea', 'problem', 'licks', 'park', 'posting', 'buying', 'steak', 'garbage', 'stupid', 'take', 'I', 'worthless', 'help', 'maybe', 'so', 'love', 'mr', 'dalmation', 'worhless', 'is', 'stop', 'food', 'quit', 'cute', 'dog', 'has', 'my']\n"
     ]
    }
   ],
   "source": [
    "myVocabList = createVocabList(listPost)\n",
    "print(myVocabList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:16:07.426072Z",
     "start_time": "2019-11-07T09:16:07.394872Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainNB0(trainMat, trainLabel):\n",
    "    numTraindoc = len(trainMat)\n",
    "    numWord = len(trainMat[0])\n",
    "    p_Abusive = sum(trainLabel) / float(numTraindoc)\n",
    "#     p0_num = np.zeros(numWord); p1_num = np.zeros(numWord) #分子\n",
    "#     p0_denom = 0.; p1_denom = 0. #分母\n",
    "# 为避免出现prob=0时导致NB结果为0，初始化变为\n",
    "    p0_num = np.ones(numWord); p1_num = np.ones(numWord)\n",
    "    p0_denom = 2.; p1_denom=2.\n",
    "#     print('p0_num, p1_num, p0_denom, p1_denom:',p0_num, p1_num, p0_denom, p1_denom)\n",
    "\n",
    "    for i in range(numTraindoc):\n",
    "        if trainLabel[i] == 1:\n",
    "            p1_num += trainMat[i]\n",
    "            p1_denom += sum(trainMat[i])\n",
    "#             print('p1_num, p1_denom:',p1_num, p1_denom)\n",
    "        else:\n",
    "            p0_num += trainMat[i]\n",
    "            p0_denom += sum(trainMat[i])\n",
    "#             print('p0_num, p0_denom:',p0_num, p0_denom)\n",
    "#     p0 = p0_num / p0_denom; p1 = p1_num / p1_denom\n",
    "# 为避免下溢，对乘积取对数 ln(a*b)=lna+lnb\n",
    "    p0 = np.log(p0_num / p0_denom); p1 = np.log(p1_num / p1_denom)\n",
    "\n",
    "    return p0, p1, p_Abusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T08:57:53.926551Z",
     "start_time": "2019-11-07T08:57:53.910951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 posts, 33\n"
     ]
    }
   ],
   "source": [
    "trainMat = []\n",
    "for post in listPost:\n",
    "    trainMat.append(Words2Vec(myVocabList, post))\n",
    "print(len(trainMat),'posts,',len(trainMat[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:01:10.908097Z",
     "start_time": "2019-11-07T09:01:10.892497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04166667 0.04166667 0.04166667 0.         0.04166667 0.08333333\n",
      " 0.04166667 0.04166667 0.04166667 0.         0.         0.\n",
      " 0.04166667 0.         0.         0.         0.04166667 0.\n",
      " 0.04166667 0.         0.04166667 0.04166667 0.04166667 0.04166667\n",
      " 0.         0.04166667 0.04166667 0.         0.         0.04166667\n",
      " 0.04166667 0.04166667 0.125     ]\n",
      "prob_absive: 0.5\n"
     ]
    }
   ],
   "source": [
    "p0, p1, pAb = trainNB0(trainMat, listClass)\n",
    "print(p0)\n",
    "print('prob_absive:', pAb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:08:54.416111Z",
     "start_time": "2019-11-07T09:08:54.400511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. 14th word (prob=0.15789473684210525) can mostly represent for Category 1.\n",
      "This word is \"stupid\"\n"
     ]
    }
   ],
   "source": [
    "index = list(p1).index(max(p1))\n",
    "print('No. %dth word (prob=%s) can mostly represent for Category 1.' % (index, max(p1)))\n",
    "print('This word is \"%s\"' % myVocabList[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classify NB and test NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:26:42.893188Z",
     "start_time": "2019-11-07T09:26:42.877588Z"
    }
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass):\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass)\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1. - pClass)\n",
    "    if p1 > p0: return 1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:44:43.039085Z",
     "start_time": "2019-11-07T09:44:43.007885Z"
    }
   },
   "outputs": [],
   "source": [
    "def testNB(testEntry, myVocabList=myVocabList):\n",
    "    testDoc = Words2Vec(myVocabList, testEntry)\n",
    "    \n",
    "    listPost, listClass = loadDataSet()\n",
    "    trainMat = []\n",
    "    for post in listPost: trainMat.append(Words2Vec(myVocabList, post))\n",
    "    p0V, p1V, pAb = trainNB0(trainMat, listClass)\n",
    "    \n",
    "    print(testEntry,'is classified as', classifyNB(testDoc, p0V, p1V, pAb))\n",
    "\n",
    "# ⬇️写的不对吧？\n",
    "def testNB_bag(testEntry, myVocabList=myVocabList):\n",
    "    testDoc = bagWords2Vec(myVocabList, testEntry)\n",
    "    \n",
    "    listPost, listClass = loadDataSet()\n",
    "    trainMat = []\n",
    "    for post in listPost: trainMat.append(bagWords2Vec(myVocabList, post))\n",
    "    p0V, p1V, pAb = trainNB0(trainMat, listClass)\n",
    "    \n",
    "    print(testEntry,'is classified as', classifyNB(testDoc, p0V, p1V, pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:42:57.021299Z",
     "start_time": "2019-11-07T09:42:57.005699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] is classified as 0\n",
      "['love', 'my', 'dalmation'] is classified as 0\n"
     ]
    }
   ],
   "source": [
    "testEntry = ['love', 'my', 'dalmation']\n",
    "testNB(testEntry)\n",
    "testNB_bag(testEntry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T09:43:47.346987Z",
     "start_time": "2019-11-07T09:43:47.331387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stupid', 'garbage', 'stupid'] is classified as 1\n",
      "['stupid', 'garbage', 'stupid'] is classified as 1\n"
     ]
    }
   ],
   "source": [
    "testE = ['stupid', 'garbage', 'stupid']#,'darling']\n",
    "testNB(testE)\n",
    "testNB_bag(testE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB过滤垃圾邮件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data prepare: slice the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T12:52:30.455275Z",
     "start_time": "2019-11-07T12:52:30.424075Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将text分割成词汇列表\n",
    "def textParse(bigString):\n",
    "    import re #正则表达式包\n",
    "    listTokens = re.split(r'\\W*', bigString) #reg=re.compile('\\\\W*') lis=reg.split(bigString)\n",
    "    # 过滤掉长度<3的字符串(因为里面包含url地址)，自首字母大写的，全部改成小写词汇\n",
    "    return [tok.lower() for tok in listTokens if len(tok)>2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T14:13:31.845014Z",
     "start_time": "2019-11-07T14:13:31.829414Z"
    }
   },
   "outputs": [],
   "source": [
    "def spamTest():\n",
    "    docList = []; classList = []; fullText = []\n",
    "    for i in range(1, 26):\n",
    "        wordList = textParse(open('./MLiAc4_spam/%d.txt'%i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1) #垃圾邮件\n",
    "        \n",
    "        wordList = textParse(open('./MLiAc4_ham/%d.txt'%i, encoding='windows-1252').read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0) #正常邮件\n",
    "    \n",
    "    vocabList = createVocabList(docList) #单词表\n",
    "    \n",
    "    # 划分训练集和测试集\n",
    "    trainSet = list(range(50)) #因为数字就是文档名称\n",
    "    testSet = []\n",
    "#     print(len(trainSet), trainSet)\n",
    "    \n",
    "    # 随机选取10个用于测试→重复运行取平均既是交叉验证\n",
    "    for i in range(10):\n",
    "        randIndex = int(np.random.uniform(0, len(trainSet)))\n",
    "        testSet.append(trainSet[randIndex])\n",
    "        del(trainSet[randIndex])\n",
    "    \n",
    "    # train NB\n",
    "    trainMat = []; trainClass = []\n",
    "    for docIndex in trainSet:\n",
    "        trainMat.append(Words2Vec(vocabList, docList[docIndex]))\n",
    "        trainClass.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClass))\n",
    "    \n",
    "    # test NB: error rate\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVec = Words2Vec(vocabList, docList[docIndex])\n",
    "        predictClass = classifyNB(np.array(wordVec), p0V, p1V, pSpam)\n",
    "        if predictClass != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print(\"File %d class is %d, misclassified as %d.\"%(docIndex, classList[docIndex], predictClass))\n",
    "    \n",
    "    err_rate = float(errorCount/len(testSet))\n",
    "    print(\"\\terror rate is %f \\n\"%err_rate)\n",
    "    \n",
    "    return err_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T14:13:08.803773Z",
     "start_time": "2019-11-07T14:13:08.772573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "l = [1,2,3]\n",
    "print(np.average(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T14:13:33.436216Z",
     "start_time": "2019-11-07T14:13:33.233416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Test 1/10\n",
      "File 21 class is 0, misclassified as 1.\n",
      "File 15 class is 0, misclassified as 1.\n",
      "File 37 class is 0, misclassified as 1.\n",
      "File 19 class is 0, misclassified as 1.\n",
      "File 5 class is 0, misclassified as 1.\n",
      "File 11 class is 0, misclassified as 1.\n",
      "File 23 class is 0, misclassified as 1.\n",
      "\terror rate is 0.700000 \n",
      "\n",
      "Cross Validation Test 2/10\n",
      "File 12 class is 1, misclassified as 0.\n",
      "File 0 class is 1, misclassified as 0.\n",
      "File 8 class is 1, misclassified as 0.\n",
      "File 30 class is 1, misclassified as 0.\n",
      "File 36 class is 1, misclassified as 0.\n",
      "\terror rate is 0.500000 \n",
      "\n",
      "Cross Validation Test 3/10\n",
      "File 28 class is 1, misclassified as 0.\n",
      "File 36 class is 1, misclassified as 0.\n",
      "File 26 class is 1, misclassified as 0.\n",
      "File 44 class is 1, misclassified as 0.\n",
      "File 34 class is 1, misclassified as 0.\n",
      "\terror rate is 0.500000 \n",
      "\n",
      "Cross Validation Test 4/10\n",
      "File 11 class is 0, misclassified as 1.\n",
      "File 49 class is 0, misclassified as 1.\n",
      "File 9 class is 0, misclassified as 1.\n",
      "File 39 class is 0, misclassified as 1.\n",
      "File 29 class is 0, misclassified as 1.\n",
      "File 47 class is 0, misclassified as 1.\n",
      "File 19 class is 0, misclassified as 1.\n",
      "\terror rate is 0.700000 \n",
      "\n",
      "Cross Validation Test 5/10\n",
      "File 40 class is 1, misclassified as 0.\n",
      "File 44 class is 1, misclassified as 0.\n",
      "File 12 class is 1, misclassified as 0.\n",
      "File 32 class is 1, misclassified as 0.\n",
      "File 10 class is 1, misclassified as 0.\n",
      "File 36 class is 1, misclassified as 0.\n",
      "File 8 class is 1, misclassified as 0.\n",
      "\terror rate is 0.700000 \n",
      "\n",
      "Cross Validation Test 6/10\n",
      "File 27 class is 0, misclassified as 1.\n",
      "File 3 class is 0, misclassified as 1.\n",
      "File 15 class is 0, misclassified as 1.\n",
      "File 21 class is 0, misclassified as 1.\n",
      "File 23 class is 0, misclassified as 1.\n",
      "File 45 class is 0, misclassified as 1.\n",
      "\terror rate is 0.600000 \n",
      "\n",
      "Cross Validation Test 7/10\n",
      "File 3 class is 0, misclassified as 1.\n",
      "File 47 class is 0, misclassified as 1.\n",
      "File 33 class is 0, misclassified as 1.\n",
      "File 25 class is 0, misclassified as 1.\n",
      "File 1 class is 0, misclassified as 1.\n",
      "File 43 class is 0, misclassified as 1.\n",
      "File 37 class is 0, misclassified as 1.\n",
      "\terror rate is 0.700000 \n",
      "\n",
      "Cross Validation Test 8/10\n",
      "File 31 class is 0, misclassified as 1.\n",
      "File 35 class is 0, misclassified as 1.\n",
      "File 13 class is 0, misclassified as 1.\n",
      "File 15 class is 0, misclassified as 1.\n",
      "File 37 class is 0, misclassified as 1.\n",
      "File 27 class is 0, misclassified as 1.\n",
      "File 23 class is 0, misclassified as 1.\n",
      "\terror rate is 0.700000 \n",
      "\n",
      "Cross Validation Test 9/10\n",
      "File 32 class is 1, misclassified as 0.\n",
      "File 18 class is 1, misclassified as 0.\n",
      "File 6 class is 1, misclassified as 0.\n",
      "File 40 class is 1, misclassified as 0.\n",
      "File 28 class is 1, misclassified as 0.\n",
      "\terror rate is 0.500000 \n",
      "\n",
      "Cross Validation Test 10/10\n",
      "File 36 class is 1, misclassified as 0.\n",
      "File 48 class is 1, misclassified as 0.\n",
      "File 14 class is 1, misclassified as 0.\n",
      "File 8 class is 1, misclassified as 0.\n",
      "File 40 class is 1, misclassified as 0.\n",
      "\terror rate is 0.500000 \n",
      "\n",
      "The average error rate of 10 times corss validation is 61.00%\n"
     ]
    }
   ],
   "source": [
    "# test 10 times to get an average error rate\n",
    "def CrossValid(times=10):\n",
    "    errorlist = []\n",
    "    for i in range(times):\n",
    "        print(\"Cross Validation Test %d/%d\"%(i+1,times))\n",
    "        errorlist.append(spamTest())\n",
    "#     print(errorlist)\n",
    "#     print(len(errorlist))\n",
    "    err_avg = np.average(errorlist)\n",
    "    print(\"The average error rate of %d times corss validation is %.2f%%\"%(times, float(err_avg)*100))\n",
    "    return err_avg\n",
    "\n",
    "CrossValid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
